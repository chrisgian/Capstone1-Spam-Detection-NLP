{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMS Spam Detection for App\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import warnings\n",
    "import gensim as gs\n",
    "import nltk \n",
    "from re import sub # import sub to replace items in the followiong list comprehension\n",
    "from collections import defaultdict\n",
    "from sklearn.lda import LDA\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn import linear_model\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = [\"I can probably do something this week? I am almos convinced on idea lol\"]\n",
    "external_data = pd.DataFrame({'content':new_doc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. Prep Data\n",
    "def prep_nlp(data_to_prep, stop_words_in, symbols_to_remove):\n",
    "    # lower case it\n",
    "    clean = list(data_to_prep.str.lower())\n",
    "    # this will tokenize\n",
    "    clean = [[word for word in document.split()] for document in clean]\n",
    "    words_to_remove = '|'.join(stop_words_in)\n",
    "    symbol_remover = '[^A-Za-z0-9]+'\n",
    "    clean = [[sub(symbol_remover,'',word) for word in text] for text in clean]\n",
    "    clean = [[sub(words_to_remove,'',word) for word in text] for text in clean]\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Build unsupervised Model\n",
    "1. Build term frequencies.\n",
    "2. Build dictionray using term frequencies\n",
    "3. Build Corpus /  bag of words \n",
    "4. Create term frequency inverse document frequency matrix\n",
    "5. Reduce matrix into 300 topics (dimension reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(train_data, topic_n):\n",
    "    frequency = defaultdict(int)\n",
    "    for text in train_data:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "    # get freq > 1\n",
    "    word_freq_1plus = [[x for x in words if frequency[x] > 1] for words in train_data]\n",
    "    # Create dictionary\n",
    "    dictionary = gs.corpora.Dictionary(word_freq_1plus)\n",
    "    # Create Corpus\n",
    "    corpus = [dictionary.doc2bow(text) for text in train_data]\n",
    "    # corpus to tfidf\n",
    "    tfidf = gs.models.TfidfModel(corpus) \n",
    "    corp_tf = tfidf[corpus] \n",
    "    # Unsupervised Component. Reduce space into 300 topics. \n",
    "    topic_n = topic_n\n",
    "    lsi = gs.models.LsiModel(corp_tf, id2word=dictionary, num_topics = topic_n)\n",
    "    corp_topics = lsi[corp_tf] \n",
    "    return corp_topics, dictionary, tfidf, lsi    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Build supervised Model\n",
    "Take the unsupervised results from LSI (a matrix of 300 topics). Train against outcome variable \"Spam\" or \"Ham\" (1 or 0). \n",
    "Use Linear Discriminant Analysis to fit data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(topic_vec):\n",
    "    x = pd.DataFrame([dict(row) for row in topic_vec[0]])\n",
    "    y = (train[\"outcome\"] == \"spam\").astype(int) \n",
    "    lda = LDA()\n",
    "    mask = np.array([~np.isnan(row).any() for row in x.values])\n",
    "    x_masked = x[mask]\n",
    "    y_masked = y[mask]\n",
    "    lda = lda.fit(x_masked,y_masked)\n",
    "    return lda,x_masked,y_masked, topic_vec[1],topic_vec[2], topic_vec[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Test Model on Unseen Data\n",
    "This sets up the function \"Predict Unseen\" which takes documents which were not in the origial training set. This can be used for validation data, test, or any additional documents. The following steps are applied:\n",
    "1. Run the new documents through the same preparation steps as training. \n",
    "2. Create bag of words with new data\n",
    "3. Transform into term-frequency, inverse document frequency matrix\n",
    "4. Apply results from latent semantic indexing and remove missing values\n",
    "5. Predict classes based on LSI results into class \"Spam\" or \"Ham\" (1 or 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict_unseen(new_doc_in, stop_words_in, trained_model_in, symbols_to_remove, y_given = True):\n",
    "\n",
    "    dictionary_in = trained_model_in[3]\n",
    "    tfidf_in = trained_model_in[4]\n",
    "    lsi_in = trained_model_in[5]\n",
    "    lda_in = trained_model_in[0]\n",
    "    new_doc_in_content = pd.Series(new_doc_in.content)\n",
    "    \n",
    "    \n",
    "    query = prep_nlp(new_doc_in_content, stop_words_in, symbols_to_remove)\n",
    "    query_bow = [dictionary_in.doc2bow(corp) for corp in query]\n",
    "    query_tf = tfidf_in[query_bow] \n",
    "    \n",
    "    x_2 = pd.DataFrame([dict(tf) for tf in lsi_in[query_tf]])\n",
    "    \n",
    "    if y_given == True: \n",
    "        new_doc_in_outcome = pd.Series(new_doc_in.outcome)\n",
    "        mask = np.array([~np.isnan(row).any() for row in x_2.values])\n",
    "        x_2masked = x_2[mask]\n",
    "        y_2 = (new_doc_in_outcome == \"spam\").astype(int) \n",
    "        y_2masked = np.array(y_2[mask])\n",
    "        x_2masked = lda_in.predict(x_2masked)\n",
    "        return x_2masked,y_2masked\n",
    "    else:\n",
    "        return lda_in.predict(x_2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.Performance\n",
    "There are three performance metrics:\n",
    "1. \"Accuracy\" which tells us, what percent of predicted results equal the actual results\n",
    "2. \"Precision\": Of all all observations we predicted as spam, what is actually spam?\n",
    "3. \"Recall\": Of all observations actually spam, what percent did we predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def performance(result_x, result_y):\n",
    "    actual_positive = result_y == 1\n",
    "    actual_negative = result_y ==0\n",
    "    true_positives = result_x[actual_positive] == 1\n",
    "    false_positives = result_x[actual_negative] == 1\n",
    "    true_negatives = result_x[actual_negative] == 0\n",
    "    false_negatives = result_x[actual_positive] == 0\n",
    "    #A. Accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "    #B. Precision = TP/(TP + FP)\n",
    "    #C. Recall = TP/(TP + FN)\n",
    "    accuracy = sum((result_x == result_y))/len(result_y)\n",
    "    precision = sum(true_positives) / (sum(true_positives) + sum(false_positives))\n",
    "    recall = sum(true_positives) / (sum(true_positives) + sum(false_negatives))\n",
    "    return [accuracy, precision, recall, len(result_x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "retrain = False\n",
    "if retrain == True:\n",
    "        # 1. Split Data into \n",
    "    n = 3000\n",
    "    train = data.sample(n, random_state = 111)\n",
    "    test = data[~data.index.isin(train.index)]\n",
    "    validate = test.sample(1000,random_state = 111)\n",
    "    test= test[~test.index.isin(validate.index)]\n",
    "    split_correctly = 0 == sum(validate.index.isin(test.index)) + sum(test.index.isin(train.index)) + sum(validate.index.isin(train.index))\n",
    "    set_n_sizes = 'N\\'s in .. train:', train.shape,'test:', test.shape,'validate:', validate.shape\n",
    "    print('Data Split Correct?', split_correctly, '\\n'*2, set_n_sizes)\n",
    "    data = pd.read_table('SMSSpamCollection',header= None, names = ('outcome', 'content'))\n",
    "    stopwords_set1 = set(nltk.corpus.stopwords.words('english'))\n",
    "    stopwords_set2 = set('for a of the and to in or'.split())\n",
    "    stopwords_set3 = ''\n",
    "    symbol_removed1 = '[^A-Za-z0-9]+'\n",
    "    train_prepped = prep_nlp(data_to_prep = train.content,stop_words_in= stopwords_set2,symbols_to_remove=symbol_removed1)\n",
    "    built_model = build_model(train_data = train_prepped,topic_n = 300)\n",
    "    trained_model = train_model(topic_vec = built_model)\n",
    "    pickle.dump(trained_model, open( \"trained.p\", \"wb\" ) )\n",
    "else:\n",
    "    trained_model = pickle.load( open( \"trained.p\", \"rb\" ) )        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_external = predict_unseen(\n",
    "    new_doc_in=external_data,\n",
    "    stop_words_in = stopwords_set2,\n",
    "    symbols_to_remove = stopwords_set2,\n",
    "    trained_model_in = trained_model,y_given = False\n",
    ")\n",
    "predicted_external"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}