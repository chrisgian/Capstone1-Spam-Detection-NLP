{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Detection\n",
    "The purpose of this ipython note book is to: \n",
    "1. Use Latent Semantic Indexing to reduce a training set of text messages\n",
    "2. Use Linear Discriminant Analysis to predict \"topics\" into spam or ham (1 or 0)\n",
    "3. Do exploratory data analysis to inform tuning\n",
    "4. Use results to predict on test, validation, external datasets\n",
    "5. tune model for highest performance (based on accuracy, precision, and recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cglan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Slow version of gensim.models.doc2vec is being used\n",
      "C:\\Users\\Cglan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\lda.py:6: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import warnings\n",
    "import gensim as gs\n",
    "import nltk \n",
    "from re import sub # import sub to replace items in the followiong list comprehension\n",
    "from collections import defaultdict\n",
    "from sklearn.lda import LDA\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn import linear_model\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read Data:\n",
    "1. Read Spam data\n",
    "2. Read personal text messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = [\"hey dude where are you\", \"text 444 for a promotional treat\", \"dont know what time it is\", \"Our records indicate your Pension is under performing to see higher growth and up to 25% cash release reply PENSION for a free review. To opt out reply STOP\",           \"To start the process please reply YES. To opt out text STOP\", \"i'm going to be 10 mins late\"]\n",
    "external_data = pd.DataFrame({'content':new_doc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data of Spam and non spam data from UC Irvine's Spam Repository\n",
    "data = pd.read_table('SMSSpamCollection',header= None, names = ('outcome', 'content'))\n",
    "# Read my personal text messages \n",
    "\n",
    "# Stop words\n",
    "stopwords_set1 = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords_set2 = set('for a of the and to in or'.split())\n",
    "stopwords_set3 = ''\n",
    "symbol_removed1 = '[^A-Za-z0-9]+'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Preprocessing\n",
    "1. Split Train, Test, Validate, External\n",
    "2. This function will apply stopwords and remove symbols. There are two sets of stopwords and one symbol set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Split Correct? True \n",
      "\n",
      " (\"N's in .. train:\", (3000, 2), 'test:', (1572, 2), 'validate:', (1000, 2))\n"
     ]
    }
   ],
   "source": [
    "# 1. Split Data into \n",
    "n = 3000\n",
    "train = data.sample(n, random_state = 111)\n",
    "test = data[~data.index.isin(train.index)]\n",
    "validate = test.sample(1000,random_state = 111)\n",
    "test= test[~test.index.isin(validate.index)]\n",
    "split_correctly = 0 == sum(validate.index.isin(test.index)) + sum(test.index.isin(train.index)) + sum(validate.index.isin(train.index))\n",
    "set_n_sizes = 'N\\'s in .. train:', train.shape,'test:', test.shape,'validate:', validate.shape\n",
    "print('Data Split Correct?', split_correctly, '\\n'*2, set_n_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. Prep Data\n",
    "def prep_nlp(data_to_prep, stop_words_in, symbols_to_remove):\n",
    "    # lower case it\n",
    "    clean = list(data_to_prep.str.lower())\n",
    "    # this will tokenize\n",
    "    clean = [[word for word in document.split()] for document in clean]\n",
    "    words_to_remove = '|'.join(stop_words_in)\n",
    "    symbol_remover = '[^A-Za-z0-9]+'\n",
    "    clean = [[sub(symbol_remover,'',word) for word in text] for text in clean]\n",
    "    clean = [[sub(words_to_remove,'',word) for word in text] for text in clean]\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.  Data Exploration /  Feature Selection\n",
    "[Contents Located Here](https://github.com/chrisgian/SB8-Statistical-Inference/blob/master/8.6%20-%20inference%20on%20capstone/8.6%20Apply%20inferential%20statistics%20to%20Capstone%20Project.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Build unsupervised Model\n",
    "1. Build term frequencies.\n",
    "2. Build dictionray using term frequencies\n",
    "3. Build Corpus /  bag of words \n",
    "4. Create term frequency inverse document frequency matrix\n",
    "5. Reduce matrix into 300 topics (dimension reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(train_data, topic_n):\n",
    "    frequency = defaultdict(int)\n",
    "    for text in train_data:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "    # get freq > 1\n",
    "    word_freq_1plus = [[x for x in words if frequency[x] > 1] for words in train_data]\n",
    "    # Create dictionary\n",
    "    dictionary = gs.corpora.Dictionary(word_freq_1plus)\n",
    "    # Create Corpus\n",
    "    corpus = [dictionary.doc2bow(text) for text in train_data]\n",
    "    # corpus to tfidf\n",
    "    tfidf = gs.models.TfidfModel(corpus) \n",
    "    corp_tf = tfidf[corpus] \n",
    "    # Unsupervised Component. Reduce space into 300 topics. \n",
    "    topic_n = topic_n\n",
    "    lsi = gs.models.LsiModel(corp_tf, id2word=dictionary, num_topics = topic_n)\n",
    "    corp_topics = lsi[corp_tf] \n",
    "    return corp_topics, dictionary, tfidf, lsi    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Build supervised Model\n",
    "Take the unsupervised results from LSI (a matrix of 300 topics). Train against outcome variable \"Spam\" or \"Ham\" (1 or 0). \n",
    "Use Linear Discriminant Analysis to fit data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(topic_vec):\n",
    "    x = pd.DataFrame([dict(row) for row in topic_vec[0]])\n",
    "    y = (train[\"outcome\"] == \"spam\").astype(int) \n",
    "    lda = LDA()\n",
    "    mask = np.array([~np.isnan(row).any() for row in x.values])\n",
    "    x_masked = x[mask]\n",
    "    y_masked = y[mask]\n",
    "    lda = lda.fit(x_masked,y_masked)\n",
    "    return lda,x_masked,y_masked, topic_vec[1],topic_vec[2], topic_vec[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Test Model on Unseen Data\n",
    "This sets up the function \"Predict Unseen\" which takes documents which were not in the origial training set. This can be used for validation data, test, or any additional documents. The following steps are applied:\n",
    "1. Run the new documents through the same preparation steps as training. \n",
    "2. Create bag of words with new data\n",
    "3. Transform into term-frequency, inverse document frequency matrix\n",
    "4. Apply results from latent semantic indexing and remove missing values\n",
    "5. Predict classes based on LSI results into class \"Spam\" or \"Ham\" (1 or 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict_unseen(new_doc_in, stop_words_in, trained_model_in, symbols_to_remove, y_given = True):\n",
    "\n",
    "    dictionary_in = trained_model_in[3]\n",
    "    tfidf_in = trained_model_in[4]\n",
    "    lsi_in = trained_model_in[5]\n",
    "    lda_in = trained_model_in[0]\n",
    "    new_doc_in_content = pd.Series(new_doc_in.content)\n",
    "    \n",
    "    \n",
    "    query = prep_nlp(new_doc_in_content, stop_words_in, symbols_to_remove)\n",
    "    query_bow = [dictionary_in.doc2bow(corp) for corp in query]\n",
    "    query_tf = tfidf_in[query_bow] \n",
    "    \n",
    "    x_2 = pd.DataFrame([dict(tf) for tf in lsi_in[query_tf]])\n",
    "    \n",
    "    if y_given == True: \n",
    "        new_doc_in_outcome = pd.Series(new_doc_in.outcome)\n",
    "        mask = np.array([~np.isnan(row).any() for row in x_2.values])\n",
    "        x_2masked = x_2[mask]\n",
    "        y_2 = (new_doc_in_outcome == \"spam\").astype(int) \n",
    "        y_2masked = np.array(y_2[mask])\n",
    "        x_2masked = lda_in.predict(x_2masked)\n",
    "        return x_2masked,y_2masked\n",
    "    else:\n",
    "        return lda_in.predict(x_2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.Performance\n",
    "There are three performance metrics:\n",
    "1. \"Accuracy\" which tells us, what percent of predicted results equal the actual results\n",
    "2. \"Precision\": Of all all observations we predicted as spam, what is actually spam?\n",
    "3. \"Recall\": Of all observations actually spam, what percent did we predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def performance(result_x, result_y):\n",
    "    actual_positive = result_y == 1\n",
    "    actual_negative = result_y ==0\n",
    "    true_positives = result_x[actual_positive] == 1\n",
    "    false_positives = result_x[actual_negative] == 1\n",
    "    true_negatives = result_x[actual_negative] == 0\n",
    "    false_negatives = result_x[actual_positive] == 0\n",
    "    #A. Accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "    #B. Precision = TP/(TP + FP)\n",
    "    #C. Recall = TP/(TP + FN)\n",
    "    accuracy = sum((result_x == result_y))/len(result_y)\n",
    "    precision = sum(true_positives) / (sum(true_positives) + sum(false_positives))\n",
    "    recall = sum(true_positives) / (sum(true_positives) + sum(false_negatives))\n",
    "    return [accuracy, precision, recall, len(result_x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Execute Models\n",
    "\n",
    "1. Run Prep Model  \n",
    "2. Build unsupervised model  \n",
    "3. Fit Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_prepped = prep_nlp(data_to_prep = train.content,stop_words_in= stopwords_set2,symbols_to_remove=symbol_removed1)\n",
    "\n",
    "built_model = build_model(train_data = train_prepped,topic_n = 300)\n",
    "\n",
    "trained_model = train_model(topic_vec = built_model)\n",
    "\n",
    "predicted_external = predict_unseen(\n",
    "    new_doc_in=external_data,\n",
    "    stop_words_in = stopwords_set2,\n",
    "    symbols_to_remove = stopwords_set2,\n",
    "    trained_model_in = trained_model,y_given = False\n",
    ")\n",
    "predicted_external"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
