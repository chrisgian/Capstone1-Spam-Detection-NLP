{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Importing Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Slow version of gensim.models.doc2vec is being used\n",
      "C:\\Users\\Cglan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\lda.py:6: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import gensim as gs\n",
    "#import slugify as sg\n",
    "import nltk\n",
    "from re import sub # import sub to replace items in the followiong list comprehension\n",
    "from collections import defaultdict\n",
    "from sklearn.lda import LDA\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from scipy.stats import ttest_ind\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Read Data, Split into \"Train\", \"Test\", \"Validate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_table('SMSSpamCollection',header= None, names = ('outcome', 'content'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Split Correct? True \n",
      "\n",
      " (\"N's in .. train:\", (3000, 2), 'test:', (1572, 2), 'validate:', (1000, 2))\n"
     ]
    }
   ],
   "source": [
    "n = 3000\n",
    "train = data.sample(n)\n",
    "test = data[~data.index.isin(train.index)]\n",
    "validate = test.sample(1000)\n",
    "test= test[~test.index.isin(validate.index)]\n",
    "split_correctly = 0 == sum(validate.index.isin(test.index)) + sum(test.index.isin(train.index)) + sum(validate.index.isin(train.index))\n",
    "set_n_sizes = 'N\\'s in .. train:', train.shape,'test:', test.shape,'validate:', validate.shape\n",
    "\n",
    "print(\n",
    "    'Data Split Correct?',\n",
    "    split_correctly,\n",
    "    '\\n'*2,\n",
    "    set_n_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prep_nlp(data_to_prep, stop_words_in):\n",
    "    # lower case it\n",
    "    clean = list(data_to_prep.str.lower())\n",
    "    # this will tokenize\n",
    "    clean = [[word for word in document.split()] for document in clean]\n",
    "    #stopwords_set1 = set(nltk.corpus.stopwords.words('english'))\n",
    "    words_to_remove = '|'.join(stopwords_set3)\n",
    "    symbol_remover = '[^A-Za-z0-9]+'\n",
    "    clean = [[sub(symbol_remover,'',word) for word in text] for text in clean]\n",
    "    clean = [[sub(words_to_remove,'',word) for word in text] for text in clean]\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords_set2 = set('for a of the and to in or'.split())\n",
    "stopwords_set3 = ''\n",
    "train_prepped = prep_nlp(data_to_prep = train.content, stop_words_in= stopwords_set2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration /  Feature Selection\n",
    "[Contents Located Here](https://github.com/chrisgian/SB8-Statistical-Inference/blob/master/8.6%20-%20inference%20on%20capstone/8.6%20Apply%20inferential%20statistics%20to%20Capstone%20Project.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Build unsupervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(train_data, topic_n):\n",
    "    frequency = defaultdict(int)\n",
    "    for text in train_data:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "    # get freq > 1\n",
    "    word_freq_1plus = [[x for x in words if frequency[x] > 1] for words in train_data]\n",
    "    # Create dictionary\n",
    "    dictionary = gs.corpora.Dictionary(word_freq_1plus)\n",
    "    # Create Corpus\n",
    "    corpus = [dictionary.doc2bow(text) for text in train_data]\n",
    "    # corpus to tfidf\n",
    "    tfidf = gs.models.TfidfModel(corpus) \n",
    "    corp_tf = tfidf[corpus] \n",
    "    # Unsupervised Component. Reduce space into 300 topics. \n",
    "    topic_n = topic_n\n",
    "    lsi = gs.models.LsiModel(corp_tf, id2word=dictionary, num_topics = topic_n)\n",
    "    corp_topics = lsi[corp_tf] \n",
    "    return corp_topics, dictionary, tfidf, lsi    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "built_model = build_model(train_data = train_prepped, topic_n = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Build supervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(topic_vec):\n",
    "    x = pd.DataFrame([dict(row) for row in topic_vec[0]])\n",
    "    y = (train[\"outcome\"] == \"spam\").astype(int) \n",
    "    lda = LDA()\n",
    "    mask = np.array([~np.isnan(row).any() for row in x.values])\n",
    "    x_masked = x[mask]\n",
    "    y_masked = y[mask]\n",
    "    lda = lda.fit(x_masked,y_masked)\n",
    "    return lda,x_masked,y_masked, topic_vec[1],topic_vec[2], topic_vec[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98329435349148009"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model = train_model(topic_vec  = built_model)\n",
    "sum(trained_model[0].predict(trained_model[1])==trained_model[2])/len(trained_model[2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Test Model on Unseen Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict_unseen(new_doc_in, stop_words_in, trained_model_in):\n",
    "    \n",
    "    dictionary_in = trained_model_in[3]\n",
    "    tfidf_in = trained_model_in[4]\n",
    "    lsi_in = trained_model_in[5]\n",
    "    lda_in = trained_model_in[0]\n",
    "    new_doc_in_content = pd.Series(new_doc_in.content)\n",
    "    new_doc_in_outcome = pd.Series(new_doc_in.outcome)\n",
    "    \n",
    "    query = prep_nlp(new_doc_in_content, stop_words_in= stop_words_in)\n",
    "    query_bow = [dictionary_in.doc2bow(corp) for corp in query]\n",
    "    query_tf = tfidf_in[query_bow] \n",
    "    \n",
    "    x_2 = pd.DataFrame([dict(tf) for tf in lsi_in[query_tf]])\n",
    "    mask = np.array([~np.isnan(row).any() for row in x_2.values])\n",
    "    x_2masked = x_2[mask]\n",
    "    y_2 = (new_doc_in_outcome == \"spam\").astype(int) \n",
    "    \n",
    "    y_2masked = np.array(y_2[mask])\n",
    "    x_2masked = lda_in.predict(x_2masked)\n",
    "    \n",
    "    return x_2masked,y_2masked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_doc = [\"hey dude where are you\",\n",
    "           \"text 444 for a promotional treat\",\n",
    "           \"dont know what time it is\", \n",
    "           \"Our records indicate your Pension is under performing to see higher growth and up to 25% cash release reply PENSION for a free review. To opt out reply STOP\",\n",
    "          \"To start the process please reply YES. To opt out text STOP\",\n",
    "          \"i'm going to be 10 mins late\"]\n",
    "new_doc_results = ['ham','spam','ham','spam','spam','ham']\n",
    "external_data = pd.DataFrame({'content':new_doc, 'outcome':new_doc_results})\n",
    "\n",
    "predicted_test = predict_unseen(new_doc_in=test, stop_words_in = stopwords_set2, trained_model_in = trained_model)\n",
    "predicted_external = predict_unseen(new_doc_in=external_data, stop_words_in = stopwords_set2, trained_model_in = trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.Performance\n",
    "There are three performance metrics:\n",
    "1. \"Accuracy\" which tells us, what percent of predicted results equal the actual results\n",
    "2. \"Precision\": Of all all observations we predicted as spam, what is actually spam?\n",
    "3. \"Recall\": Of all observations actually spam, what percent did we predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def performance(result_x, result_y):\n",
    "    \n",
    "    actual_positive = result_y == 1\n",
    "    actual_negative = result_y ==0\n",
    "    \n",
    "    true_positives = result_x[actual_positive] == 1\n",
    "    false_positives = result_x[actual_negative] == 1\n",
    "    true_negatives = result_x[actual_negative] == 0\n",
    "    false_negatives = result_x[actual_positive] == 0\n",
    "    \n",
    "    #A. Accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "    #B. Precision = TP/(TP + FP)\n",
    "    #C. Recall = TP/(TP + FN)\n",
    "    accuracy = sum((result_x == result_y))/len(result_y)\n",
    "    precision = sum(true_positives) / (sum(true_positives) + sum(false_positives))\n",
    "    recall = sum(true_positives) / (sum(true_positives) + sum(false_negatives))\n",
    "    return [accuracy, precision, recall, len(result_x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cglan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:6: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n",
      "C:\\Users\\Cglan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n",
      "C:\\Users\\Cglan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n",
      "C:\\Users\\Cglan\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    }
   ],
   "source": [
    "performance_on_train = performance(result_x=trained_model[0].predict(trained_model[1]),result_y=trained_model[2])\n",
    "performance_on_test = performance(result_x=predicted_test[0],result_y=predicted_test[1])\n",
    "performance_on_external = performance(result_x=predicted_external[0],result_y=predicted_external[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Test</th>\n",
       "      <th>Train</th>\n",
       "      <th>external</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <th>% Spam / Ham Correct</th>\n",
       "      <td>0.975734</td>\n",
       "      <td>0.983294</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <th>% Predicted Spam Actually Spam</th>\n",
       "      <td>0.962617</td>\n",
       "      <td>0.873705</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <th>% Spam Detected</th>\n",
       "      <td>0.872881</td>\n",
       "      <td>0.873705</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N Size</th>\n",
       "      <th></th>\n",
       "      <td>1566.000000</td>\n",
       "      <td>2993.000000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Test        Train  external\n",
       "Accuracy  % Spam / Ham Correct               0.975734     0.983294       1.0\n",
       "Precision % Predicted Spam Actually Spam     0.962617     0.873705       1.0\n",
       "Recall    % Spam Detected                    0.872881     0.873705       1.0\n",
       "N Size                                    1566.000000  2993.000000       6.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_out = pd.DataFrame({\n",
    "    'Train':performance_on_train,\n",
    "    'Test':performance_on_test,\n",
    "    'external':performance_on_external\n",
    "\n",
    "}).set_index(\n",
    "    [['Accuracy','Precision','Recall','N Size'],\n",
    "     ['% Spam / Ham Correct','% Predicted Spam Actually Spam','% Spam Detected','']])\n",
    "results_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Confidence Interval for estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What kind of accuracy, precision, and recall do we expect to see in future samples given that our text messages are acquired and do not differ from the 5000 text messages utilized here? \n",
    "\n",
    "Approach: Using resampling, resample 100 text messages 10,000 times. Use these to build a sampling distribution for Accuracy, Precision, and Recall that we will see with 90% liklihood in future cases. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two functions below:\n",
    "    a. Samples 100 text messages and sorts them into the TFIDF, then predicts class membership\n",
    "    b. Iterates the first point 10,000 in order to plot, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_predictions(sample_n):    \n",
    "    predict_i = predict_unseen(new_doc_in=test.sample(sample_n), stop_words_in = stopwords_set2, trained_model_in = trained_model)\n",
    "    return performance(result_x=predict_i[0],result_y=predict_i[1]) \n",
    "\n",
    "    \n",
    "def sample_predict_distributions(sample_n, iters):\n",
    "    stats = [sample_predictions(sample_n) for i in range(iters)]\n",
    "    return stats\n",
    "\n",
    "sample_predictions_iterated = sample_predict_distributions(100,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampled_results_df = pd.DataFrame(sample_predictions_iterated)\n",
    "sampled_results_df.columns = ['Accuracy','Precision','Recall','N-Size']\n",
    "sampled_results_df = sampled_results_df\n",
    "\n",
    "def ci_generate(measure):\n",
    "    sampled_results_df[measure].plot(kind='hist')\n",
    "    low,high, estimate = sampled_results_df[measure].quantile(.05),sampled_results_df[measure].quantile(.95),sampled_results_df[measure].mean()\n",
    "    low_line = plt.plot([low, low], [0, 2500], 'k-', lw=1)\n",
    "    high_line = plt.plot([high, high], [0, 2500], 'k-', lw=1)\n",
    "    estimate_line = plt.plot([estimate,estimate],[0,2500],'k-',lw=1)\n",
    "    return low_line, high_line, print('the 90% CI for ',measure,':',estimate,' is between ',low, ' and ', high,'. Where the measurement error is ',round(estimate-low,3))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results\n",
    "For the three metrics\n",
    "- Accuracy has a range of .97 to 1\n",
    "- Recall has a range of .66  to 1\n",
    "- Precision has a range of .83 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 90% CI for  Accuracy : 0.9758883057557632  is between  0.95  and  1.0 . Where the measurement error is  0.026\n"
     ]
    }
   ],
   "source": [
    "_ = ci_generate('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 90% CI for  Recall : 0.8739089633673349  is between  0.7272727272727273  and  1.0 . Where the measurement error is  0.147\n"
     ]
    }
   ],
   "source": [
    "_ = ci_generate('Recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 90% CI for  Precision : 0.9627479321803593  is between  0.8636363636363636  and  1.0 . Where the measurement error is  0.099\n"
     ]
    }
   ],
   "source": [
    "_ = ci_generate('Precision')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
