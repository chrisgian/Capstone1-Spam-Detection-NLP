{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Detection\n",
    "The purpose of this ipython note book is to: \n",
    "1. Use Latent Semantic Indexing to reduce a training set of text messages\n",
    "2. Use Linear Discriminant Analysis to predict \"topics\" into spam or ham (1 or 0)\n",
    "3. Do exploratory data analysis to inform tuning\n",
    "4. Use results to predict on test, validation, external datasets\n",
    "5. tune model for highest performance (based on accuracy, precision, and recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import warnings\n",
    "import gensim as gs\n",
    "import nltk \n",
    "from re import sub # import sub to replace items in the followiong list comprehension\n",
    "from collections import defaultdict\n",
    "from sklearn.lda import LDA\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn import linear_model\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read Data:\n",
    "1. Read Spam data\n",
    "2. Read personal text messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data of Spam and non spam data from UC Irvine's Spam Repository\n",
    "data = pd.read_table('SMSSpamCollection',header= None, names = ('outcome', 'content'))\n",
    "# Read my personal text messages \n",
    "new_doc = [\"hey dude where are you\", \"text 444 for a promotional treat\", \"dont know what time it is\", \"Our records indicate your Pension is under performing to see higher growth and up to 25% cash release reply PENSION for a free review. To opt out reply STOP\",           \"To start the process please reply YES. To opt out text STOP\", \"i'm going to be 10 mins late\"]\n",
    "new_doc_results = ['ham','spam','ham','spam','spam','ham']\n",
    "external_data = pd.DataFrame({'content':new_doc, 'outcome':new_doc_results})\n",
    "\n",
    "# Stop words\n",
    "stopwords_set1 = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords_set2 = set('for a of the and to in or'.split())\n",
    "stopwords_set3 = ''\n",
    "symbol_removed1 = '[^A-Za-z0-9]+'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Preprocessing\n",
    "1. Split Train, Test, Validate, External\n",
    "2. This function will apply stopwords and remove symbols. There are two sets of stopwords and one symbol set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Split Correct? True \n",
      "\n",
      " (\"N's in .. train:\", (3000, 2), 'test:', (1572, 2), 'validate:', (1000, 2))\n"
     ]
    }
   ],
   "source": [
    "# 1. Split Data into \n",
    "n = 3000\n",
    "train = data.sample(n, random_state = 111)\n",
    "test = data[~data.index.isin(train.index)]\n",
    "validate = test.sample(1000,random_state = 111)\n",
    "test= test[~test.index.isin(validate.index)]\n",
    "split_correctly = 0 == sum(validate.index.isin(test.index)) + sum(test.index.isin(train.index)) + sum(validate.index.isin(train.index))\n",
    "set_n_sizes = 'N\\'s in .. train:', train.shape,'test:', test.shape,'validate:', validate.shape\n",
    "print('Data Split Correct?', split_correctly, '\\n'*2, set_n_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. Prep Data\n",
    "def prep_nlp(data_to_prep, stop_words_in, symbols_to_remove):\n",
    "    # lower case it\n",
    "    clean = list(data_to_prep.str.lower())\n",
    "    # this will tokenize\n",
    "    clean = [[word for word in document.split()] for document in clean]\n",
    "    words_to_remove = '|'.join(stop_words_in)\n",
    "    symbol_remover = '[^A-Za-z0-9]+'\n",
    "    clean = [[sub(symbol_remover,'',word) for word in text] for text in clean]\n",
    "    clean = [[sub(words_to_remove,'',word) for word in text] for text in clean]\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.  Data Exploration /  Feature Selection\n",
    "[Contents Located Here](https://github.com/chrisgian/SB8-Statistical-Inference/blob/master/8.6%20-%20inference%20on%20capstone/8.6%20Apply%20inferential%20statistics%20to%20Capstone%20Project.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Build unsupervised Model\n",
    "1. Build term frequencies.\n",
    "2. Build dictionray using term frequencies\n",
    "3. Build Corpus /  bag of words \n",
    "4. Create term frequency inverse document frequency matrix\n",
    "5. Reduce matrix into 300 topics (dimension reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(train_data, topic_n):\n",
    "    frequency = defaultdict(int)\n",
    "    for text in train_data:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "    # get freq > 1\n",
    "    word_freq_1plus = [[x for x in words if frequency[x] > 1] for words in train_data]\n",
    "    # Create dictionary\n",
    "    dictionary = gs.corpora.Dictionary(word_freq_1plus)\n",
    "    # Create Corpus\n",
    "    corpus = [dictionary.doc2bow(text) for text in train_data]\n",
    "    # corpus to tfidf\n",
    "    tfidf = gs.models.TfidfModel(corpus) \n",
    "    corp_tf = tfidf[corpus] \n",
    "    # Unsupervised Component. Reduce space into 300 topics. \n",
    "    topic_n = topic_n\n",
    "    lsi = gs.models.LsiModel(corp_tf, id2word=dictionary, num_topics = topic_n)\n",
    "    corp_topics = lsi[corp_tf] \n",
    "    return corp_topics, dictionary, tfidf, lsi    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Build supervised Model\n",
    "Take the unsupervised results from LSI (a matrix of 300 topics). Train against outcome variable \"Spam\" or \"Ham\" (1 or 0). \n",
    "Use Linear Discriminant Analysis to fit data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(topic_vec):\n",
    "    x = pd.DataFrame([dict(row) for row in topic_vec[0]])\n",
    "    y = (train[\"outcome\"] == \"spam\").astype(int) \n",
    "    lda = LDA()\n",
    "    mask = np.array([~np.isnan(row).any() for row in x.values])\n",
    "    x_masked = x[mask]\n",
    "    y_masked = y[mask]\n",
    "    lda = lda.fit(x_masked,y_masked)\n",
    "    return lda,x_masked,y_masked, topic_vec[1],topic_vec[2], topic_vec[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Test Model on Unseen Data\n",
    "This sets up the function \"Predict Unseen\" which takes documents which were not in the origial training set. This can be used for validation data, test, or any additional documents. The following steps are applied:\n",
    "1. Run the new documents through the same preparation steps as training. \n",
    "2. Create bag of words with new data\n",
    "3. Transform into term-frequency, inverse document frequency matrix\n",
    "4. Apply results from latent semantic indexing and remove missing values\n",
    "5. Predict classes based on LSI results into class \"Spam\" or \"Ham\" (1 or 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict_unseen(new_doc_in, stop_words_in, trained_model_in, symbols_to_remove):\n",
    "\n",
    "    dictionary_in = trained_model_in[3]\n",
    "    tfidf_in = trained_model_in[4]\n",
    "    lsi_in = trained_model_in[5]\n",
    "    lda_in = trained_model_in[0]\n",
    "    new_doc_in_content = pd.Series(new_doc_in.content)\n",
    "    new_doc_in_outcome = pd.Series(new_doc_in.outcome)\n",
    "    \n",
    "    query = prep_nlp(new_doc_in_content, stop_words_in, symbols_to_remove)\n",
    "    query_bow = [dictionary_in.doc2bow(corp) for corp in query]\n",
    "    query_tf = tfidf_in[query_bow] \n",
    "    \n",
    "    x_2 = pd.DataFrame([dict(tf) for tf in lsi_in[query_tf]])\n",
    "    mask = np.array([~np.isnan(row).any() for row in x_2.values])\n",
    "    x_2masked = x_2[mask]\n",
    "    y_2 = (new_doc_in_outcome == \"spam\").astype(int) \n",
    "    \n",
    "    y_2masked = np.array(y_2[mask])\n",
    "    x_2masked = lda_in.predict(x_2masked)\n",
    "    \n",
    "    return x_2masked,y_2masked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.Performance\n",
    "There are three performance metrics:\n",
    "1. \"Accuracy\" which tells us, what percent of predicted results equal the actual results\n",
    "2. \"Precision\": Of all all observations we predicted as spam, what is actually spam?\n",
    "3. \"Recall\": Of all observations actually spam, what percent did we predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(result_x, result_y):\n",
    "    actual_positive = result_y == 1\n",
    "    actual_negative = result_y ==0\n",
    "    true_positives = result_x[actual_positive] == 1\n",
    "    false_positives = result_x[actual_negative] == 1\n",
    "    true_negatives = result_x[actual_negative] == 0\n",
    "    false_negatives = result_x[actual_positive] == 0\n",
    "    #A. Accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "    #B. Precision = TP/(TP + FP)\n",
    "    #C. Recall = TP/(TP + FN)\n",
    "    accuracy = sum((result_x == result_y))/len(result_y)\n",
    "    precision = sum(true_positives) / (sum(true_positives) + sum(false_positives))\n",
    "    recall = sum(true_positives) / (sum(true_positives) + sum(false_negatives))\n",
    "    return [accuracy, precision, recall, len(result_x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Execute Models\n",
    "\n",
    "1. Run Prep Model  \n",
    "2. Build unsupervised model  \n",
    "3. Fit Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_prepped = prep_nlp(\n",
    "    data_to_prep = train.content,\n",
    "    stop_words_in= stopwords_set2,\n",
    "    symbols_to_remove=stopwords_set2)\n",
    "built_model = build_model(\n",
    "    train_data = train_prepped,\n",
    "    topic_n = 300)\n",
    "trained_model = train_model(\n",
    "    topic_vec = built_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "performance_on_train = performance(\n",
    "    result_x=trained_model[0].predict(trained_model[1]),\n",
    "    result_y=np.array(trained_model2[2]))\n",
    "predicted_test = predict_unseen(\n",
    "    new_doc_in=test,\n",
    "    stop_words_in = stopwords_set2,\n",
    "    symbols_to_remove = stopwords_set2,\n",
    "    trained_model_in = trained_model)\n",
    "performance_on_test = performance(\n",
    "    result_x=predicted_test[0],\n",
    "    result_y=predicted_test[1])\n",
    "predicted_external = predict_unseen(\n",
    "    new_doc_in=external_data,\n",
    "    stop_words_in = stopwords_set2,\n",
    "    symbols_to_remove = stopwords_set2,\n",
    "    trained_model_in = trained_model)\n",
    "performance_on_external = performance(\n",
    "    result_x=predicted_external[0],\n",
    "    result_y=predicted_external[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Test</th>\n",
       "      <th>Train</th>\n",
       "      <th>external</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <th>% Spam / Ham Correct</th>\n",
       "      <td>0.976418</td>\n",
       "      <td>0.981296</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <th>% Predicted Spam Actually Spam</th>\n",
       "      <td>0.941860</td>\n",
       "      <td>0.983740</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <th>% Spam Detected</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.878935</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N Size</th>\n",
       "      <th></th>\n",
       "      <td>1569.000000</td>\n",
       "      <td>2994.000000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Test        Train  external\n",
       "Accuracy  % Spam / Ham Correct               0.976418     0.981296       1.0\n",
       "Precision % Predicted Spam Actually Spam     0.941860     0.983740       1.0\n",
       "Recall    % Spam Detected                    0.857143     0.878935       1.0\n",
       "N Size                                    1569.000000  2994.000000       6.0"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_out = pd.DataFrame({\n",
    "    'Train':performance_on_train,\n",
    "    'Test':performance_on_test,\n",
    "    'external':performance_on_external\n",
    "}).set_index(\n",
    "    [['Accuracy','Precision','Recall','N Size'],\n",
    "     ['% Spam / Ham Correct','% Predicted Spam Actually Spam','% Spam Detected','']])\n",
    "results_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Parameter Tuning\n",
    "##### Different Stopword Lists\n",
    "Goal: Find out how different stopwords lists affect accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Custom Stopwords, w/ symbols</th>\n",
       "      <th>NLTK Stopwords, w/ symbols</th>\n",
       "      <th>None Removed, w/ symbols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <th>% Spam / Ham Correct</th>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.957958</td>\n",
       "      <td>0.969819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <th>% Predicted Spam Actually Spam</th>\n",
       "      <td>0.945736</td>\n",
       "      <td>0.917355</td>\n",
       "      <td>0.952000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <th>% Spam Detected</th>\n",
       "      <td>0.853147</td>\n",
       "      <td>0.776224</td>\n",
       "      <td>0.832168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N Size</th>\n",
       "      <th></th>\n",
       "      <td>994.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>994.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Custom Stopwords, w/ symbols  \\\n",
       "Accuracy  % Spam / Ham Correct                                0.971831   \n",
       "Precision % Predicted Spam Actually Spam                      0.945736   \n",
       "Recall    % Spam Detected                                     0.853147   \n",
       "N Size                                                      994.000000   \n",
       "\n",
       "                                          NLTK Stopwords, w/ symbols  \\\n",
       "Accuracy  % Spam / Ham Correct                              0.957958   \n",
       "Precision % Predicted Spam Actually Spam                    0.917355   \n",
       "Recall    % Spam Detected                                   0.776224   \n",
       "N Size                                                    999.000000   \n",
       "\n",
       "                                          None Removed, w/ symbols  \n",
       "Accuracy  % Spam / Ham Correct                            0.969819  \n",
       "Precision % Predicted Spam Actually Spam                  0.952000  \n",
       "Recall    % Spam Detected                                 0.832168  \n",
       "N Size                                                  994.000000  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_prepped1 = prep_nlp(data_to_prep = train.content,stop_words_in= stopwords_set1,symbols_to_remove=stopwords_set3)\n",
    "train_prepped2 = prep_nlp(data_to_prep = train.content,stop_words_in= stopwords_set2,symbols_to_remove=stopwords_set3)\n",
    "train_prepped3 = prep_nlp(data_to_prep = train.content,stop_words_in= stopwords_set3,symbols_to_remove=stopwords_set3)\n",
    "built_model1 = build_model(train_data = train_prepped1,topic_n = 300)\n",
    "built_model2 = build_model(train_data = train_prepped2,topic_n = 300)\n",
    "built_model3 = build_model(train_data = train_prepped3,topic_n = 300)\n",
    "trained_model1 = train_model(topic_vec = built_model1)\n",
    "trained_model2 = train_model(topic_vec = built_model2)\n",
    "trained_model3 = train_model(topic_vec = built_model3)\n",
    "predicted_test1 = predict_unseen(new_doc_in=validate,stop_words_in = stopwords_set1,symbols_to_remove = symbol_removed1,trained_model_in = trained_model1)\n",
    "predicted_test2 = predict_unseen(new_doc_in=validate,stop_words_in = stopwords_set2,symbols_to_remove = symbol_removed1,trained_model_in = trained_model2)\n",
    "predicted_test3 = predict_unseen(new_doc_in=validate,stop_words_in = stopwords_set3,symbols_to_remove = symbol_removed1,trained_model_in = trained_model3)\n",
    "results_out = pd.DataFrame({\n",
    "    'NLTK Stopwords, w/ symbols':performance(result_x=predicted_test1[0], result_y=predicted_test1[1]),\n",
    "    'Custom Stopwords, w/ symbols':performance(result_x=predicted_test2[0], result_y=predicted_test2[1]),\n",
    "    'None Removed, w/ symbols':performance(result_x=predicted_test3[0], result_y=predicted_test3[1])\n",
    "}).set_index(\n",
    "    [['Accuracy','Precision','Recall','N Size'],\n",
    "     ['% Spam / Ham Correct','% Predicted Spam Actually Spam','% Spam Detected','']])\n",
    "results_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Custom Stopwords, w/o symbols</th>\n",
       "      <th>NLTK Stopwords, w/o symbols</th>\n",
       "      <th>None Removed, w/o symbols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <th>% Spam / Ham Correct</th>\n",
       "      <td>0.969819</td>\n",
       "      <td>0.958959</td>\n",
       "      <td>0.967807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <th>% Predicted Spam Actually Spam</th>\n",
       "      <td>0.944882</td>\n",
       "      <td>0.918033</td>\n",
       "      <td>0.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <th>% Spam Detected</th>\n",
       "      <td>0.839161</td>\n",
       "      <td>0.783217</td>\n",
       "      <td>0.825175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N Size</th>\n",
       "      <th></th>\n",
       "      <td>994.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>994.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Custom Stopwords, w/o symbols  \\\n",
       "Accuracy  % Spam / Ham Correct                                 0.969819   \n",
       "Precision % Predicted Spam Actually Spam                       0.944882   \n",
       "Recall    % Spam Detected                                      0.839161   \n",
       "N Size                                                       994.000000   \n",
       "\n",
       "                                          NLTK Stopwords, w/o symbols  \\\n",
       "Accuracy  % Spam / Ham Correct                               0.958959   \n",
       "Precision % Predicted Spam Actually Spam                     0.918033   \n",
       "Recall    % Spam Detected                                    0.783217   \n",
       "N Size                                                     999.000000   \n",
       "\n",
       "                                          None Removed, w/o symbols  \n",
       "Accuracy  % Spam / Ham Correct                             0.967807  \n",
       "Precision % Predicted Spam Actually Spam                   0.944000  \n",
       "Recall    % Spam Detected                                  0.825175  \n",
       "N Size                                                   994.000000  "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_prepped1 = prep_nlp(data_to_prep = train.content,stop_words_in= stopwords_set1,symbols_to_remove=symbol_removed1)\n",
    "train_prepped2 = prep_nlp(data_to_prep = train.content,stop_words_in= stopwords_set2,symbols_to_remove=symbol_removed1)\n",
    "train_prepped3 = prep_nlp(data_to_prep = train.content,stop_words_in= stopwords_set3,symbols_to_remove=symbol_removed1)\n",
    "built_model1 = build_model(train_data = train_prepped1,topic_n = 300)\n",
    "built_model2 = build_model(train_data = train_prepped2,topic_n = 300)\n",
    "built_model3 = build_model(train_data = train_prepped3,topic_n = 300)\n",
    "trained_model1 = train_model(topic_vec = built_model1)\n",
    "trained_model2 = train_model(topic_vec = built_model2)\n",
    "trained_model3 = train_model(topic_vec = built_model3)\n",
    "predicted_test1 = predict_unseen(new_doc_in=validate,stop_words_in = stopwords_set1,symbols_to_remove = symbol_removed1,trained_model_in = trained_model1)\n",
    "predicted_test2 = predict_unseen(new_doc_in=validate,stop_words_in = stopwords_set2,symbols_to_remove = symbol_removed1,trained_model_in = trained_model2)\n",
    "predicted_test3 = predict_unseen(new_doc_in=validate,stop_words_in = stopwords_set3,symbols_to_remove = symbol_removed1,trained_model_in = trained_model3)\n",
    "results_out = pd.DataFrame({\n",
    "    'NLTK Stopwords, w/o symbols':performance(result_x=predicted_test1[0], result_y=predicted_test1[1]),\n",
    "    'Custom Stopwords, w/o symbols':performance(result_x=predicted_test2[0], result_y=predicted_test2[1]),\n",
    "    'None Removed, w/o symbols':performance(result_x=predicted_test3[0], result_y=predicted_test3[1])\n",
    "}).set_index(\n",
    "    [['Accuracy','Precision','Recall','N Size'],\n",
    "     ['% Spam / Ham Correct','% Predicted Spam Actually Spam','% Spam Detected','']])\n",
    "results_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning Number of Topics\n",
    "Goal: Identify how different number of topics impact performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_at_topic_i(topic_i):\n",
    "    built_model = build_model(train_data = train_prepped, topic_n = topic_i)\n",
    "    trained_model = train_model(topic_vec  = built_model)\n",
    "    predicted_test = predict_unseen(new_doc_in=test, stop_words_in = stopwords_set2, trained_model_in = trained_model)\n",
    "    performance_on_test = performance(result_x=predicted_test[0],result_y=predicted_test[1])\n",
    "    return performance_on_test\n",
    "\n",
    "def model_iterator(start,end):\n",
    "    model_iteration = [model_at_topic_i(i) for i in range(start,end+1)]\n",
    "    return model_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict_unseen() missing 1 required positional argument: 'symbols_to_remove'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-206-eb5b8aed82c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtopic_100_to_500\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-205-73a3d4c79a8a>\u001b[0m in \u001b[0;36mmodel_iterator\u001b[0;34m(start, end)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmodel_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodel_iteration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmodel_at_topic_i\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel_iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-205-73a3d4c79a8a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmodel_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodel_iteration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmodel_at_topic_i\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel_iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-205-73a3d4c79a8a>\u001b[0m in \u001b[0;36mmodel_at_topic_i\u001b[0;34m(topic_i)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mbuilt_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_prepped\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopic_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrained_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_vec\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mbuilt_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpredicted_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_unseen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_doc_in\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords_set2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrained_model_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mperformance_on_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperformance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredicted_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresult_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredicted_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mperformance_on_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: predict_unseen() missing 1 required positional argument: 'symbols_to_remove'"
     ]
    }
   ],
   "source": [
    "topic_100_to_500 = model_iterator(start = 100, end = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topics_frame = pd.DataFrame(topic_100_to_500)\n",
    "topics_frame.columns = ['Accuracy','Precision','Recall','N-Size']\n",
    "topics_frame['topics'] = range(100,501)\n",
    "\n",
    "sns.regplot(\n",
    "    x='topics',\n",
    "    y='Accuracy',\n",
    "    data = topics_frame[['topics','Accuracy']], \n",
    "    fit_reg=True,\n",
    "    x_bins=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning Conclusions:\n",
    "- Stopwords: From the results above, the highest accuracy seems to occur when no stopwords are used. This is not surprising as the hypothesis tests seem to validate that spam text messages use more stopwords. This can be because spam messages generally include full sentences and grammatically correct structures in order to create readable messages that advertisers can get across to consumers. Because of this -- I will not use stopwords. \n",
    "- Number of Topics: 300 Seems to be a turning point and gains seem to cease after 400, thus 400 seems to be a good candidate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Confidence Interval for estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What kind of accuracy, precision, and recall do we expect to see in future samples given that our text messages are acquired and do not differ from the 5000 text messages utilized here? \n",
    "\n",
    "Approach: Using resampling, resample 100 text messages 10,000 times. Use these to build a sampling distribution for Accuracy, Precision, and Recall that we will see with 90% liklihood in future cases. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two functions below:\n",
    "    a. Samples 100 text messages and sorts them into the TFIDF, then predicts class membership\n",
    "    b. Iterates the first point 10,000 in order to plot, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict_unseen() missing 1 required positional argument: 'symbols_to_remove'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-fb4af3779483>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msample_predictions_iterated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_predict_distributions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-fb4af3779483>\u001b[0m in \u001b[0;36msample_predict_distributions\u001b[0;34m(sample_n, iters)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msample_predict_distributions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msample_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_n\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-fb4af3779483>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msample_predict_distributions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msample_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_n\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-fb4af3779483>\u001b[0m in \u001b[0;36msample_predictions\u001b[0;34m(sample_n)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msample_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpredict_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_unseen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_doc_in\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords_set2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrained_model_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mperformance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredict_i\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresult_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredict_i\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: predict_unseen() missing 1 required positional argument: 'symbols_to_remove'"
     ]
    }
   ],
   "source": [
    "def sample_predictions(sample_n):    \n",
    "    predict_i = predict_unseen(new_doc_in=test.sample(sample_n), stop_words_in = stopwords_set2, trained_model_in = trained_model)\n",
    "    return performance(result_x=predict_i[0],result_y=predict_i[1]) \n",
    "\n",
    "    \n",
    "def sample_predict_distributions(sample_n, iters):\n",
    "    stats = [sample_predictions(sample_n) for i in range(iters)]\n",
    "    return stats\n",
    "\n",
    "sample_predictions_iterated = sample_predict_distributions(100,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampled_results_df = pd.DataFrame(sample_predictions_iterated)\n",
    "sampled_results_df.columns = ['Accuracy','Precision','Recall','N-Size']\n",
    "sampled_results_df = sampled_results_df\n",
    "\n",
    "def ci_generate(measure):\n",
    "    sampled_results_df[measure].plot(kind='hist')\n",
    "    low,high, estimate = sampled_results_df[measure].quantile(.05),sampled_results_df[measure].quantile(.95),sampled_results_df[measure].mean()\n",
    "    low_line = plt.plot([low, low], [0, 2500], 'k-', lw=1)\n",
    "    high_line = plt.plot([high, high], [0, 2500], 'k-', lw=1)\n",
    "    estimate_line = plt.plot([estimate,estimate],[0,2500],'k-',lw=1)\n",
    "    return low_line, high_line, print('the 90% CI for ',measure,':',estimate,' is between ',low, ' and ', high,'. Where the measurement error is ',round(estimate-low,3))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results\n",
    "For the three metrics\n",
    "- Accuracy has a range of .97 to 1\n",
    "- Recall has a range of .66  to 1\n",
    "- Precision has a range of .83 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = ci_generate('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = ci_generate('Recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = ci_generate('Precision')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
